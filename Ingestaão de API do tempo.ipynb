{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b17ab86-521f-483e-84ee-f4860d5fe933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# --- 1. CONFIGURAÇÃO ---\n",
    "API_KEY = \"f4a68bb2393847b82a7b667d4f4ae537\"  # <--- COLOQUE SUA CHAVE\n",
    "BASE_URL = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "\n",
    "# Lista completa das 27 capitais brasileiras\n",
    "capitais = [\n",
    "    \"Rio Branco\", \"Maceió\", \"Macapá\", \"Manaus\", \"Salvador\", \"Fortaleza\", \"Brasília\",\n",
    "    \"Vitória\", \"Goiânia\", \"São Luís\", \"Cuiabá\", \"Campo Grande\", \"Belo Horizonte\",\n",
    "    \"Belém\", \"João Pessoa\", \"Curitiba\", \"Recife\", \"Teresina\", \"Rio de Janeiro\",\n",
    "    \"Natal\", \"Porto Alegre\", \"Porto Velho\", \"Boa Vista\", \"Florianópolis\",\n",
    "    \"São Paulo\", \"Aracaju\", \"Palmas\"\n",
    "]\n",
    "\n",
    "lista_dados_coletados = [] # Lista vazia para guardar os resultados\n",
    "\n",
    "print(f\"Iniciando coleta de {len(capitais)} capitais...\")\n",
    "\n",
    "# --- 2. O LOOP (EXTRAÇÃO) ---\n",
    "for cidade in capitais:\n",
    "    \n",
    "    # Montamos os parâmetros, forçando \",BR\" para garantir que é no Brasil\n",
    "    params = {\n",
    "        \"q\": f\"{cidade},BR\", \n",
    "        \"appid\": API_KEY,\n",
    "        \"units\": \"metric\",\n",
    "        \"lang\": \"pt_br\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # Extraindo só o que importa (Flattening)\n",
    "            registro = {\n",
    "                \"cidade\": data[\"name\"],\n",
    "                \"estado_sigla\": \"BR\", # A API as vezes não manda o estado preciso, mas sabemos que é BR\n",
    "                \"temperatura\": data[\"main\"][\"temp\"],\n",
    "                \"sensacao\": data[\"main\"][\"feels_like\"],\n",
    "                \"umidade\": data[\"main\"][\"humidity\"],\n",
    "                \"clima\": data[\"weather\"][0][\"description\"],\n",
    "                \"latitude\": data[\"coord\"][\"lat\"],\n",
    "                \"longitude\": data[\"coord\"][\"lon\"]\n",
    "            }\n",
    "            \n",
    "            lista_dados_coletados.append(registro)\n",
    "            print(f\"✔ {cidade}: OK\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"❌ Erro em {cidade}: {response.status_code}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Erro crítico em {cidade}: {str(e)}\")\n",
    "\n",
    "# --- 3. TRANSFORMAÇÃO E CARGA ---\n",
    "\n",
    "# Só prossegue se tiver coletado algo\n",
    "if lista_dados_coletados:\n",
    "    \n",
    "    # Cria o DataFrame Pandas com TODOS os dados de uma vez\n",
    "    df_pandas = pd.DataFrame(lista_dados_coletados)\n",
    "    \n",
    "    # Converte para Spark\n",
    "    df_spark = spark.createDataFrame(df_pandas)\n",
    "    \n",
    "    # Adiciona data de processamento\n",
    "    df_final = df_spark.withColumn(\"data_ingestao\", current_timestamp())\n",
    "    \n",
    "    # Exibe resultado\n",
    "    display(df_final)\n",
    "    \n",
    "    # Salva no Delta Lake (agora especificando o lugar certo)\n",
    "    # Formato: nome_do_schema.nome_da_tabela\n",
    "    nome_completo_tabela = \"aprender_data.tabela_clima_capitais\"\n",
    "    \n",
    "    (df_final.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .option(\"mergeSchema\", \"true\") # Garante que se a tabela mudar, ele aceita\n",
    "        .saveAsTable(nome_completo_tabela))\n",
    "    \n",
    "    print(f\"\\nSUCESSO: Todas as capitais processadas e salvas em: {nome_completo_tabela}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6fde23e-cd8e-4f8b-a77d-7206871fe19b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Cria uma \"pasta\" chamada aprender_data se ela não existir\n",
    "CREATE SCHEMA IF NOT EXISTS aprender_data;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7808293491763550,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestaão de API do tempo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
